# Exercise: Working with Datasets Using JDBC and PostgreSQL

Develop a standalone Spark SQL application that reads data from a JDBC database, e.g. PostgreSQL.

Module: **Spark SQL**

Duration: **30 mins**

## Steps

1. **spark-submit --packages** to submit Spark application with PostgreSQL JDBC Driver
2. Use **Spark Standalone** cluster
    1. Troubleshoot missing PostgreSQL JDBC Driver jar

## Useful Links

1. [Working with Datasets from JDBC Data Sources (and PostgreSQL)](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/exercises/spark-exercise-dataframe-jdbc-postgresql.html)
